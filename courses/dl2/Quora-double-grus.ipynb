{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Evaluation - using pre-trained weights from the LM/Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/aclImdb/')\n",
    "CLAS_PATH=Path('data/imdb_clas/')\n",
    "LM_PATH=Path('data/imdb_lm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz=400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a standard dataset specifically meant for the semantic similarity task.\n",
    "Let's use the quora kaggle dataset that contains pairs of english sentences and the goal is to predict if a given pair of sentences are semantically similar(meaning).\n",
    "\n",
    "y=1 indicates they have the same meaning  \n",
    "y=0 means the pair differ in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question pairs: 404290\n"
     ]
    }
   ],
   "source": [
    "QUESTION_PAIRS_FILE = 'data/quora_duplicate_questions.tsv'\n",
    "#print(\"Processing\", QUESTION_PAIRS_FILE)\n",
    "\n",
    "question1 = []\n",
    "question2 = []\n",
    "is_duplicate = []\n",
    "with open(QUESTION_PAIRS_FILE, encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        question1.append(row['question1'])\n",
    "        question2.append(row['question2'])\n",
    "        is_duplicate.append(int(row['is_duplicate']))\n",
    "\n",
    "print('Question pairs: %d' % len(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the step by step guide to invest in share market in india?',\n",
       " 'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
       " 'How can I increase the speed of my internet connection while using a VPN?',\n",
       " 'Why am I mentally very lonely? How can I solve it?',\n",
       " 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?',\n",
       " 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
       " 'Should I buy tiago?',\n",
       " 'How can I be a good geologist?',\n",
       " 'When do you use シ instead of し?',\n",
       " 'Motorola (company): Can I hack my Charter Motorolla DCX3400?']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the step by step guide to invest in share market?',\n",
       " 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?',\n",
       " 'How can Internet speed be increased by hacking through DNS?',\n",
       " 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?',\n",
       " 'Which fish would survive in salt water?',\n",
       " \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\",\n",
       " 'What keeps childern active and far from phone and video games?',\n",
       " 'What should I do to be a great geologist?',\n",
       " 'When do you use \"&\" instead of \"and\"?',\n",
       " 'How do I hack Motorola DCX3400 for free internet?']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 404290, 404290)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question1),len(question2),len(is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_duplicate[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since sim(A,B) = sim (B,A) we can double the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = question1 + question2\n",
    "question2 = question2 + question1\n",
    "question1 = ques\n",
    "is_duplicate = is_duplicate + is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_q1 = Tokenizer.proc_all_mp(partition_by_cores(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_q2 = Tokenizer.proc_all_mp(partition_by_cores(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'step',\n",
       "  'by',\n",
       "  'step',\n",
       "  'guide',\n",
       "  'to',\n",
       "  'invest',\n",
       "  'in',\n",
       "  'share',\n",
       "  'market',\n",
       "  '?'],\n",
       " ['what',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'if',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'government',\n",
       "  'stole',\n",
       "  'the',\n",
       "  'kohinoor',\n",
       "  '(',\n",
       "  'koh',\n",
       "  '-',\n",
       "  'i',\n",
       "  '-',\n",
       "  'noor',\n",
       "  ')',\n",
       "  'diamond',\n",
       "  'back',\n",
       "  '?'],\n",
       " ['how',\n",
       "  'can',\n",
       "  'internet',\n",
       "  'speed',\n",
       "  'be',\n",
       "  'increased',\n",
       "  'by',\n",
       "  'hacking',\n",
       "  'through',\n",
       "  't_up',\n",
       "  'dns',\n",
       "  '?'],\n",
       " ['find',\n",
       "  'the',\n",
       "  'remainder',\n",
       "  'when',\n",
       "  '[',\n",
       "  'math]23^{24',\n",
       "  '}',\n",
       "  '[',\n",
       "  '/',\n",
       "  'math',\n",
       "  ']',\n",
       "  'is',\n",
       "  'divided',\n",
       "  'by',\n",
       "  '24,23',\n",
       "  '?'],\n",
       " ['which', 'fish', 'would', 'survive', 'in', 'salt', 'water', '?'],\n",
       " ['i',\n",
       "  \"'m\",\n",
       "  'a',\n",
       "  'triple',\n",
       "  'capricorn',\n",
       "  '(',\n",
       "  'sun',\n",
       "  ',',\n",
       "  'moon',\n",
       "  'and',\n",
       "  'ascendant',\n",
       "  'in',\n",
       "  'capricorn',\n",
       "  ')',\n",
       "  'what',\n",
       "  'does',\n",
       "  'this',\n",
       "  'say',\n",
       "  'about',\n",
       "  'me',\n",
       "  '?'],\n",
       " ['what',\n",
       "  'keeps',\n",
       "  'childern',\n",
       "  'active',\n",
       "  'and',\n",
       "  'far',\n",
       "  'from',\n",
       "  'phone',\n",
       "  'and',\n",
       "  'video',\n",
       "  'games',\n",
       "  '?'],\n",
       " ['what', 'should', 'i', 'do', 'to', 'be', 'a', 'great', 'geologist', '?'],\n",
       " ['when',\n",
       "  'do',\n",
       "  'you',\n",
       "  'use',\n",
       "  '\"',\n",
       "  '&',\n",
       "  '\"',\n",
       "  'instead',\n",
       "  'of',\n",
       "  '\"',\n",
       "  'and',\n",
       "  '\"',\n",
       "  '?'],\n",
       " ['how',\n",
       "  'do',\n",
       "  'i',\n",
       "  'hack',\n",
       "  'motorola',\n",
       "  't_up',\n",
       "  'dcx3400',\n",
       "  'for',\n",
       "  'free',\n",
       "  'internet',\n",
       "  '?']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_q2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537054"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = sorted(set(map(tuple, tok_q1+tok_q2)), reverse=True)\n",
    "len(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(p for o in ques for p in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 567912),\n",
       " ('the', 251908),\n",
       " ('what', 214480),\n",
       " ('is', 186191),\n",
       " ('a', 154911),\n",
       " ('i', 149562),\n",
       " ('to', 141677),\n",
       " ('in', 139553),\n",
       " ('how', 135432),\n",
       " ('of', 111924),\n",
       " ('do', 110223),\n",
       " ('are', 98555),\n",
       " ('and', 89368),\n",
       " ('for', 74763),\n",
       " ('t_up', 73392),\n",
       " (',', 72193),\n",
       " ('can', 71248),\n",
       " ('you', 60716),\n",
       " ('why', 56154),\n",
       " ('it', 52359),\n",
       " ('my', 45133),\n",
       " ('does', 41961),\n",
       " ('best', 40964),\n",
       " ('.', 39769),\n",
       " ('on', 37977)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', '?', 'the', 'what', 'is', 'a', 'i', 'to', 'in']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37597"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', '?', 'the', 'what', 'is', 'a', 'i', 'to', 'in']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "list(stoi)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.array([[stoi[o] for o in p] for p in tok_q1])\n",
    "q2 = np.array([[stoi[o] for o in p] for p in tok_q2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((808580,), (808580,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape,q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[4, 5, 3, 1312, 69, 1312, 2275, 8, 552, 9, 684, 347, 9, 47, 2]'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(q1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in',\n",
       "       'india', '?'], dtype='<U33')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos_arr = np.array(itos)\n",
    "itos_arr[q1[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('q1_dbl.npy', q1)\n",
    "np.save('q2_dbl.npy', q2)\n",
    "pickle.dump(itos, open('itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.load('q1_dbl.npy')\n",
    "q2 = np.load('q2_dbl.npy')\n",
    "itos = pickle.load(open('itos.pkl', 'rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37597, 808580)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=len(itos) #vocab size\n",
    "vs,len(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.encoder.weight', '0.encoder_with_dropout.embed.weight', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.0.module.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '0.rnns.1.module.weight_hh_l0_raw', '0.rnns.2.module.weight_ih_l0', '0.rnns.2.module.bias_ih_l0', '0.rnns.2.module.bias_hh_l0', '0.rnns.2.module.weight_hh_l0_raw', '1.decoder.weight'])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400,),\n",
       " array([-0.0183 , -0.13826,  0.01438, -0.01285,  0.00407,  0.01944,  0.01149, -0.13282, -0.02295, -0.01722],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0) \n",
    "row_m.shape, row_m[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix and take token weights from wikitext103 if available\n",
    "# Use 60002 instead of 41665 for future embedding matrix where backbone encoder needs to be loaded.\n",
    "# not needed for simple model\n",
    "new_w = np.zeros((len(itos), em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37597, 400)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37597, 400])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['1.decoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_keep = np.random.rand(len(q1))>0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_trn = q1[trn_keep]\n",
    "q2_trn = q2[trn_keep]\n",
    "lbl_trn = np.asarray(is_duplicate)[trn_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([lbl_trn]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T(np.array([lbl_trn[101]]).T)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727447,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_val = q1[~trn_keep]\n",
    "q2_val = q2[~trn_keep]\n",
    "lbl_val = np.asarray(is_duplicate)[~trn_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81133, 1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_val = np.asarray([lbl_val]).T\n",
    "lbl_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727447, 1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_trn = np.asarray([lbl_trn]).T\n",
    "lbl_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 81133)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_val = lbl_val.T\n",
    "lbl_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 727447)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_trn = lbl_trn.T\n",
    "lbl_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37597, 400)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs,em_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, X, y): self.x1,self.x2,self.y = X[0],X[1],y\n",
    "    def __getitem__(self, idx): return A(self.x1[idx], self.x2[idx], (T(self.y[idx]).float()))\n",
    "    def __len__(self): return len(self.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = PairDataset(X=[q1_trn[:1000],q2_trn[:1000]],y=(lbl_trn[:1000]).T)\n",
    "val_ds = PairDataset(X=[q1_val[:100],q2_val[:100]],y=(lbl_val[:100]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = PairDataset(X=[q1_trn,q2_trn],y=(lbl_trn).T)\n",
    "val_ds = PairDataset(X=[q1_val,q2_val],y=(lbl_val).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  20,   12,  282,   92,    8,   30,  259,   31,    3,  264,   72, 4428,    2]),\n",
       " array([   10,    12,   598,   118,    76, 11955,     6,   142,     2]),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.__getitem__(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unable to run with larger bs because of DataLoader transpose issue\n",
    "bs=300\n",
    "#bs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False) #, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False) #, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(51, 43, 300), (37, 46, 300), (34, 48, 300), (43, 60, 300), (76, 41, 300)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(trn_dl)\n",
    "its = [next(it) for i in range(5)]\n",
    "[(len(x1),len(x2),len(y)) for x1,x2,y in its]\n",
    "#[((y)) for x1,x2,y in its]\n",
    "#next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model - simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        try: wgts[i] = torch.from_numpy(vecs[i])\n",
    "        except: miss.append(w)\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh,nl = 256,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another layer of GRUs (not bi directional) and see if we can provide alternate pathways (_ala resnet_) for the gradients to flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairRNN(nn.Module):\n",
    "    #def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "    def __init__(self, vecs, itos, em_sz, nh, out_sl=75, nl=2, bs=100):\n",
    "        super().__init__()\n",
    "        self.nl,self.nh,self.out_sl,self.bs = nl,nh,out_sl,bs\n",
    "        self.emb = create_emb(vecs, itos, em_sz)\n",
    "        self.emb_drop = nn.Dropout(0.15)\n",
    "        self.gru1 = nn.GRU(em_sz, nh, num_layers=nl, dropout=0.25)\n",
    "        self.gru2 = nn.GRU(em_sz, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out = nn.Linear(2*nh, em_sz, bias=False)\n",
    "        \n",
    "    def forward(self, inp1, inp2):\n",
    "        sl,bs = inp1.size()\n",
    "        h = self.initHidden(bs)\n",
    "\n",
    "        emb1 = self.emb_drop(self.emb(inp1))\n",
    "        emb2 = self.emb_drop(self.emb(inp2))\n",
    "        \n",
    "        out_1, h1 = self.gru1(emb1, h) #Gru layer 1\n",
    "        out_2, h2 = self.gru1(emb2, h) #Gru layer 1\n",
    "        out_3, h3 = self.gru2(emb1, h) #Gru layer 2\n",
    "        out_4, h4 = self.gru2(emb2, h) #Gru layer 2\n",
    "        \n",
    "        h5 = self.out(torch.cat((h1[1],h3[1]),1))\n",
    "        h6 = self.out(torch.cat((h2[1],h4[1]),1))\n",
    "        \n",
    "        return F.cosine_similarity(h5,h6)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We have opted to use the L1 loss to optimize. This means we are trying to get our cosine sim to predict values very close to 0 or 1 or penalize if it drifts away too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quora kaggle competition uses a BCE loss(negative log likelihood) to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n"
     ]
    }
   ],
   "source": [
    "rnn = PairRNN(new_w, itos, em_sz, nh, bs=bs)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = nn.L1Loss()\n",
    "#nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairRNN(\n",
       "  (emb): Embedding(37597, 400, padding_idx=1)\n",
       "  (emb_drop): Dropout(p=0.15)\n",
       "  (gru1): GRU(400, 256, num_layers=2, dropout=0.25)\n",
       "  (gru2): GRU(400, 256, num_layers=2, dropout=0.25)\n",
       "  (out): Linear(in_features=512, out_features=400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn[0].trainable = False\n",
    "learn[0].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                     \n",
      "    0      0.286228   0.410427  \n",
      "    1      0.257659   0.301446                                     \n",
      "    2      0.253119   0.331992                                     \n",
      "    3      0.221312   0.282643                                     \n",
      "    4      0.212439   0.264009                                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.26401])]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=5, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('quora_db1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                     \n",
      "    0      0.228651   0.266893  \n",
      "    1      0.210128   0.234721                                     \n",
      "    2      0.196772   0.230007                                     \n",
      "    3      0.18365    0.219447                                     \n",
      "    4      0.17546    0.220643                                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.22064])]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=5, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('quora_db2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn[0].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                     \n",
      "    0      0.19798    0.226221  \n",
      "    1      0.190937   0.222594                                     \n",
      "    2      0.181158   0.213164                                     \n",
      "    3      0.172567   0.207104                                     \n",
      "    4      0.16399    0.202823                                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.20282])]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=5, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('quora_db3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81133, 81133)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val),len(lbl_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.38694489295355 % accuracy\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(  (np.asarray(val,dtype=float) >= 0.7611) == (lbl_val[0])  ) / len(val) * 100, \"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                     \n",
      "    0      0.190384   0.216909  \n",
      "    1      0.182725   0.217011                                     \n",
      "    2      0.176167   0.204406                                     \n",
      "    3      0.168004   0.197842                                     \n",
      "    4      0.161245   0.191268                                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.19127])]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=5, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the L1loss function to optimize the learner. This gave us good semantic similarity scores in general. Let's now evaluate if this translates to better semantic similatity specifically for the quora dataset. The quora kaggle challenge uses negative log likelihood to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = learn.predict() # get evaluation from our L1loss trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81133, 81133)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val),len(lbl_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate negative log likelihood (BCE LOSS) from predictions\n",
    "def binary_loss(y, p):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49602077407368567"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_loss(lbl_val[0],np.clip(val,0.06,0.907))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 0.496 which is quite poor for the quora set. The competition leaderboard shows 0.1 and 0.2 log loss scores. Please see this [github page](https://github.com/lemuriandezapada/quora_test) for a much better solution.\n",
    "\n",
    "This probably means that we have captured the essence of semantic similarity using L1 loss + cosine similarity - but we have failed to tune it to the quora dataset specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to switch from L1 loss to neg log loss and tune some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('quora_db4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss_crit(y, p):\n",
    "    p = p.clamp(min=0.06,max=0.907)\n",
    "    return torch.mean(-(y * torch.log(p) + (1-y)*torch.log(1-p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.crit=binary_loss_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5864d539de4fce9fda571d33071e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                     \n",
      "    0      0.864667   0.953047  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.95305])]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "learn.fit(lr, 1, cycle_len=1, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('quora_db5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it has a long way to go as 0.953 is a terrible score. Let's abandon this approach for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_inp = [\"i like apples\",\n",
    "         \"i want to buy some apples\",\n",
    "         \"where is your cell phone\"]\n",
    "\n",
    "x2_inp = [\"i like apples and oranges\",\n",
    "         \"i love all fruits especially apples and oranges\",\n",
    "         \"where is the new movie showing?\"]\n",
    "\n",
    "x3_inp = [\"let's talk about fruits for a second. Apples are nice. Oranges too. I kinda like them.\",\n",
    "         \"i compared the prices of apples and oranges at walmart and kroger stores\",\n",
    "         \"oh you wanna talk about apples. sure. i am not sure if i have said this before but i do like them and oranges.\"]\n",
    "\n",
    "x4_inp = [\"there is no comparison here. you are comparing apples to oranges\",\n",
    "         \"i compared the prices of apples and oranges at walmart and kroger stores\",\n",
    "         \"i don't see anything common between these two categories.\"]\n",
    "\n",
    "x5_inp = [\"i would love to own a nice boat and go sailing in the pacific ocean\",\n",
    "         \"i'm thinking of getting a fancy boat and set sail into the south pacific\",\n",
    "         \"i wish to own a small house and live there without any worries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = Tokenizer().proc_all(x1_inp,'en')\n",
    "tok2 = Tokenizer().proc_all(x2_inp,'en')\n",
    "tok3 = Tokenizer().proc_all(x3_inp,'en')\n",
    "tok4 = Tokenizer().proc_all(x4_inp,'en')\n",
    "tok5 = Tokenizer().proc_all(x5_inp,'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [[stoi[o1] for o1 in o] for o in tok1]\n",
    "X2 = [[stoi[o1] for o1 in o] for o in tok2]\n",
    "X3 = [[stoi[o1] for o1 in o] for o in tok3]\n",
    "X4 = [[stoi[o1] for o1 in o] for o in tok4]\n",
    "X5 = [[stoi[o1] for o1 in o] for o in tok5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "def predict_similarities(m,sent0,sent1,sent2):\n",
    "    m.eval()\n",
    "    cc0 = m((V(T([sent0]).permute(1,0))),(V(T([sent1]).permute(1,0))))\n",
    "    cc1 = m((V(T([sent1]).permute(1,0))),(V(T([sent2]).permute(1,0))))\n",
    "    cc2 = m((V(T([sent0]).permute(1,0))),(V(T([sent2]).permute(1,0))))\n",
    "    return cc0.data[0],cc1.data[0],cc2.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('quora_db2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like apples', 'i want to buy some apples', 'where is your cell phone']"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = X1[0]; sent1 = X1[1]; sent2 = X1[2] #round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8084962964057922, 0.5435537695884705, 0.16693493723869324)\n"
     ]
    }
   ],
   "source": [
    "print (predict_similarities(learn.model,sent0,sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim results from previous model\n",
    "# 1: (0.9999998807907104, 0.06893263012170792, 0.06893263757228851)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like apples and oranges',\n",
       " 'i love all fruits especially apples and oranges',\n",
       " 'where is the new movie showing?']"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = X2[0]; sent1 = X2[1]; sent2 = X2[2] #round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2680668234825134, 0.11690233647823334, -0.020731007680296898)\n"
     ]
    }
   ],
   "source": [
    "print (predict_similarities(learn.model,sent0,sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim results from previous model\n",
    "# 2: (0.8213068842887878, 0.22487039864063263, 0.2921583652496338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"let's talk about fruits for a second. Apples are nice. Oranges too. I kinda like them.\",\n",
       " 'i compared the prices of apples and oranges at walmart and kroger stores',\n",
       " 'oh you wanna talk about apples. sure. i am not sure if i have said this before but i do like them and oranges.']"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = X3[0]; sent1 = X3[1]; sent2 = X3[2] #round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.007769078016281128, 0.2540293335914612, -0.027798207476735115)\n"
     ]
    }
   ],
   "source": [
    "print (predict_similarities(learn.model,sent0,sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim results from previous model\n",
    "# 3: (0.043320432305336, 0.043320432305336, 1.0000001192092896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is no comparison here. you are comparing apples to oranges',\n",
       " 'i compared the prices of apples and oranges at walmart and kroger stores',\n",
       " \"i don't see anything common between these two categories.\"]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = X4[0]; sent1 = X4[1]; sent2 = X4[2] #round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7291907072067261, 0.02016669511795044, 0.009204450994729996)\n"
     ]
    }
   ],
   "source": [
    "print (predict_similarities(learn.model,sent0,sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim results from previous model\n",
    "# 4: (0.0017274579731747508, 0.043320432305336, 0.02612287364900112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would love to own a nice boat and go sailing in the pacific ocean',\n",
       " \"i'm thinking of getting a fancy boat and set sail into the south pacific\",\n",
       " 'i wish to own a small house and live there without any worries']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = X5[0]; sent1 = X5[1]; sent2 = X5[2] #round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6918005347251892, 0.2942836582660675, 0.17552269995212555)\n"
     ]
    }
   ],
   "source": [
    "print (predict_similarities(learn.model,sent0,sent1,sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_sim results from previous model\n",
    "# 5: (0.5177718997001648, 0.14863775670528412, 0.046706970781087875)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
